{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "Tiny AI Chatbot",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chris-bhaila/ANAIS-2025/blob/main/Day%208%20-%20Tiny_AI_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤– Tiny AI Chatbot  \n",
        "### A Hands-On Workshop on How AI Learns to Talk\n",
        "\n",
        "![AI Chatbot Banner](https://iili.io/fNdUQ72.png)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Œ Introduction\n",
        "\n",
        "Artificial Intelligence (AI) can now **understand and generate human-like language**.  \n",
        "From chatbots and virtual assistants to automated customer support, AI systems are learning how to *talk* and this notebook shows **how that actually works**, step by step.\n",
        "\n",
        "In this notebook, we build a **simple AI chatbot** and train it using real text data. No advanced math or deep AI background is required.\n",
        "\n",
        "For a deeper understanding of how Large Language Models (LLMs) work under the hood, watch this excellent talk by Andrej Karpathy:  \n",
        "ðŸ‘‰ https://www.youtube.com/watch?v=bZQun8Y4L2A\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ What You Will Learn\n",
        "\n",
        "By the end of this session, you will understand:\n",
        "\n",
        "- How AI learns language from **text data**\n",
        "- What a **language model** is and how it works\n",
        "- The difference between **raw data** and **trained intelligence**\n",
        "- How chatbots learn to:\n",
        "  - Hold conversations  \n",
        "  - Tell stories  \n",
        "  - Solve basic math problems\n",
        "- How a trained AI model can be used through a **simple chat interface**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  What We Will Build\n",
        "\n",
        "We will build a **Tiny AI Chatbot** that can:\n",
        "\n",
        "- Chat with users naturally\n",
        "- Tell short stories\n",
        "- Answer math questions\n",
        "- Remember short conversation history\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ›  Tools & Technologies Used\n",
        "\n",
        "- **Python** â€“ programming language\n",
        "- **Hugging Face Datasets** â€“ for text data\n",
        "- **Transformers (GPT-2)** â€“ the AI language model\n",
        "- **Gradio** â€“ to create a chat interface\n",
        "\n",
        "\n",
        "\n",
        "ðŸ‘‰ Letâ€™s begin our journey!"
      ],
      "metadata": {
        "id": "H2E2TZMvh_as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required libraries\n",
        "- **datasets** â€“ to download and manage text datasets\n",
        "- **transformers** â€“ to load and train the GPT-2 language model\n",
        "- **accelerate** â€“ to optimize training performance\n",
        "- **torch** â€“ deep learning framework\n",
        "- **gradio** â€“ to create an interactive chat interface\n"
      ],
      "metadata": {
        "id": "8ak3w8cHfY8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets==2.16.0 transformers==4.57.0 accelerate==1.10.1 torch gradio==5.49.0"
      ],
      "metadata": {
        "id": "qief7JeruqCN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T10:17:01.099135Z",
          "iopub.execute_input": "2026-01-03T10:17:01.099489Z",
          "iopub.status.idle": "2026-01-03T10:17:30.783755Z",
          "shell.execute_reply.started": "2026-01-03T10:17:01.099462Z",
          "shell.execute_reply": "2026-01-03T10:17:30.783098Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "id": "l_-U9Owyq8jo",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T10:17:30.785363Z",
          "iopub.execute_input": "2026-01-03T10:17:30.785616Z",
          "iopub.status.idle": "2026-01-03T10:17:30.789338Z",
          "shell.execute_reply.started": "2026-01-03T10:17:30.785589Z",
          "shell.execute_reply": "2026-01-03T10:17:30.788792Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and view Datasets  \n",
        "**Datasets:**  \n",
        "- **DailyDialog**: https://huggingface.co/datasets/roskoN/dailydialog  \n",
        "- **TinyStories**: https://huggingface.co/datasets/roneneldan/TinyStories  \n",
        "- **AI-MO / NuminaMath-CoT**: https://huggingface.co/datasets/AI-MO/NuminaMath-CoT  "
      ],
      "metadata": {
        "id": "A7IZIGowfcxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Download dialog dataset\n",
        "chat_ds = load_dataset(\"roskoN/dailydialog\", trust_remote_code=True)\n",
        "\n",
        "print(chat_ds)\n",
        "print()\n",
        "chat_ds[\"train\"][0]"
      ],
      "metadata": {
        "id": "YLu4HmXGuvEe",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T10:17:30.790187Z",
          "iopub.execute_input": "2026-01-03T10:17:30.790464Z",
          "iopub.status.idle": "2026-01-03T10:17:40.640595Z",
          "shell.execute_reply.started": "2026-01-03T10:17:30.790434Z",
          "shell.execute_reply": "2026-01-03T10:17:40.640045Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Download story dataset\n",
        "story_ds = load_dataset(\"roneneldan/TinyStories\")\n",
        "\n",
        "print(story_ds)\n",
        "print()\n",
        "story_ds['train'][0][\"text\"]"
      ],
      "metadata": {
        "id": "dyWQYE10p7Nq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T10:17:40.642487Z",
          "iopub.execute_input": "2026-01-03T10:17:40.642962Z",
          "iopub.status.idle": "2026-01-03T10:17:56.176852Z",
          "shell.execute_reply.started": "2026-01-03T10:17:40.642934Z",
          "shell.execute_reply": "2026-01-03T10:17:56.176184Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Download math dataset\n",
        "math_ds = load_dataset(\"AI-MO/NuminaMath-CoT\")\n",
        "\n",
        "print(math_ds)\n",
        "print()\n",
        "math_ds['train'][0]"
      ],
      "metadata": {
        "id": "Zq1f5l4PyjfY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T10:17:56.17762Z",
          "iopub.execute_input": "2026-01-03T10:17:56.177921Z",
          "iopub.status.idle": "2026-01-03T10:18:26.872217Z",
          "shell.execute_reply.started": "2026-01-03T10:17:56.177891Z",
          "shell.execute_reply": "2026-01-03T10:18:26.871642Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Chat Dataset\n",
        "\n",
        "The chat dataset is enhanced by injecting stories and math problems into conversations.\n",
        "\n",
        "### Key Concepts\n",
        "- Alternating **User** and **Bot** messages\n",
        "- Random insertion of:\n",
        "  - Story prompts and responses\n",
        "  - Math problems and solutions\n",
        "- **END_TOKEN** added after each bot response\n",
        "- All prepared datasets are combined into a single training dataset."
      ],
      "metadata": {
        "id": "gz5m2Dp7l4ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "import random\n",
        "import re\n",
        "\n",
        "# Config\n",
        "# ----------------------------\n",
        "CHAT_SAMPLES  = 2000\n",
        "STORY_SAMPLES = 2000\n",
        "MATH_SAMPLES  = 2000\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "STORY_INSERT_PROB = 0.3\n",
        "MATH_INSERT_PROB = 0.3\n",
        "\n",
        "END_TOKEN = \"\\n\"\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "STORY_PROMPTS = [\n",
        "    \"Write a story.\",\n",
        "    \"Tell a story.\",\n",
        "    \"Create a story.\",\n",
        "    \"Write a short story.\",\n",
        "    \"Make up a story.\",\n",
        "    \"Share a story.\",\n",
        "    \"Invent a story.\",\n",
        "    \"Compose a story.\",\n",
        "    \"Write a narrative.\",\n",
        "    \"Create a narrative.\",\n",
        "    \"Tell a tale.\",\n",
        "    \"Write a fictional story.\",\n",
        "    \"Create a short narrative.\",\n",
        "    \"Tell an original story.\",\n",
        "    \"Write an imaginative story.\",\n",
        "    \"Compose a short tale.\",\n",
        "    \"Create an original story.\",\n",
        "    \"Write a creative story.\",\n",
        "    \"Tell a short tale.\",\n",
        "    \"Make up a short story.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Math LaTeX Normalizer (Gradio-safe)\n",
        "# -------------------------------------------------\n",
        "def normalize_math_tex(text: str) -> str:\n",
        "    # Convert \\[ ... \\] â†’ $$ ... $$\n",
        "    text = re.sub(r\"\\\\\\[(.*?)\\\\\\]\", r\"$$\\1$$\", text, flags=re.DOTALL)\n",
        "\n",
        "    # Convert single $...$ â†’ $$...$$ (ignore existing $$)\n",
        "    text = re.sub(\n",
        "        r\"(?<!\\$)\\$(?!\\$)(.+?)(?<!\\$)\\$(?!\\$)\",\n",
        "        r\"$$\\1$$\",\n",
        "        text,\n",
        "        flags=re.DOTALL,\n",
        "    )\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# Load Story Dataset\n",
        "# -------------------------------------------------\n",
        "story_raw_ds = load_dataset(\n",
        "    \"roneneldan/TinyStories\",\n",
        "    split=f\"train[:{STORY_SAMPLES}]\"\n",
        ")\n",
        "\n",
        "stories = [\n",
        "    s[\"text\"].replace(\"\\n\\n\", \" \").strip()\n",
        "    for s in story_raw_ds\n",
        "]\n",
        "\n",
        "\n",
        "# Load Math Dataset\n",
        "# -------------------------------------------------\n",
        "math_raw_ds = load_dataset(\n",
        "    \"AI-MO/NuminaMath-CoT\",\n",
        "    split=f\"train[:{MATH_SAMPLES}]\"\n",
        ")\n",
        "\n",
        "math_problems = [\n",
        "    (\n",
        "        normalize_math_tex(m[\"problem\"]),\n",
        "        normalize_math_tex(m[\"solution\"])\n",
        "    )\n",
        "    for m in math_raw_ds\n",
        "]\n",
        "\n",
        "\n",
        "# Chat Formatter (Story + Math Injection)\n",
        "# -------------------------------------------------\n",
        "def format_chat(example):\n",
        "    text = \"\"\n",
        "    utterances = example[\"utterances\"]\n",
        "\n",
        "    insert_story = random.random() < STORY_INSERT_PROB\n",
        "    insert_math = random.random() < MATH_INSERT_PROB\n",
        "\n",
        "    insert_idx = random.randrange(0, len(utterances), 2)\n",
        "\n",
        "    for i, sentence in enumerate(utterances):\n",
        "        if i % 2 == 0:\n",
        "            text += f\"User: {sentence}\\n\"\n",
        "        else:\n",
        "            text += f\"Bot: {sentence} {END_TOKEN}\\n\"\n",
        "\n",
        "        if i == insert_idx and i % 2 == 0:\n",
        "            if insert_story:\n",
        "                prompt = random.choice(STORY_PROMPTS)\n",
        "                story = random.choice(stories)\n",
        "                text += f\"User: {prompt}\\n\"\n",
        "                text += f\"Bot: {story} {END_TOKEN}\\n\"\n",
        "\n",
        "            ## Uncomment this if you want to train with math dataset\n",
        "            # elif insert_math:\n",
        "            #     problem, solution = random.choice(math_problems)\n",
        "            #     text += f\"User: {problem}\\n\"\n",
        "            #     text += f\"Bot: {solution} {END_TOKEN}\\n\"\n",
        "\n",
        "    return {\"text\": text.strip()}\n",
        "\n",
        "\n",
        "# Story-only Formatter\n",
        "# -------------------------------------------------\n",
        "def format_story(example):\n",
        "    prompt = random.choice(STORY_PROMPTS)\n",
        "    story = example[\"text\"].strip()  #.replace(\"\\n\\n\", \" \")\n",
        "    return {\n",
        "        \"text\": f\"User: {prompt}\\nBot: {story} {END_TOKEN}\"\n",
        "    }\n",
        "\n",
        "\n",
        "# Math-only Formatter\n",
        "# -------------------------------------------------\n",
        "def format_math(example):\n",
        "    return {\n",
        "        \"text\": (\n",
        "            f\"User: {normalize_math_tex(example['problem'])}\\n\"\n",
        "            f\"Bot: {normalize_math_tex(example['solution'])} {END_TOKEN}\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "\n",
        "# Load & Prepare Chat Dataset\n",
        "# -------------------------------------------------\n",
        "chat_ds = load_dataset(\n",
        "    \"roskoN/dailydialog\",\n",
        "    split=f\"train[:{CHAT_SAMPLES}]\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "chat_ds = chat_ds.map(\n",
        "    format_chat,\n",
        "    remove_columns=chat_ds.column_names\n",
        ")\n",
        "\n",
        "\n",
        "# Prepare Story-only Dataset\n",
        "# -------------------------------------------------\n",
        "story_ds = story_raw_ds.map(\n",
        "    format_story,\n",
        "    remove_columns=story_raw_ds.column_names\n",
        ")\n",
        "\n",
        "\n",
        "# Prepare Math-only Dataset\n",
        "# -------------------------------------------------\n",
        "math_ds = math_raw_ds.map(\n",
        "    format_math,\n",
        "    remove_columns=math_raw_ds.column_names\n",
        ")\n",
        "\n",
        "\n",
        "# Merge + Shuffle\n",
        "# -------------------------------------------------\n",
        "dataset = concatenate_datasets([chat_ds, story_ds])           # Train with Chat + Story dataset\n",
        "#dataset = concatenate_datasets([chat_ds, story_ds, math_ds])  # Train with Chat + Story + Math dataset\n",
        "\n",
        "dataset = dataset.shuffle(seed=SEED)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Sanity Check\n",
        "print(dataset)\n",
        "for i in range(15):\n",
        "    print(\"\\n--- SAMPLE ---\\n\")\n",
        "    print(dataset[i][\"text\"])"
      ],
      "metadata": {
        "id": "bm-Q0830l20w",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T11:47:41.517831Z",
          "iopub.execute_input": "2026-01-03T11:47:41.518144Z",
          "iopub.status.idle": "2026-01-03T11:47:49.700634Z",
          "shell.execute_reply.started": "2026-01-03T11:47:41.518118Z",
          "shell.execute_reply": "2026-01-03T11:47:49.700022Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Base Language Model\n",
        "\n",
        "A pre-trained GPT-2 model is used as the foundation.\n",
        "\n",
        "### Why GPT-2?\n",
        "- Already understands basic language patterns\n",
        "- Small enough for fast training\n",
        "\n",
        "The tokenizer and model are configured to support custom tokens."
      ],
      "metadata": {
        "id": "NKv_yT3dfkQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "FQAlXn-NuvXg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T11:47:49.701896Z",
          "iopub.execute_input": "2026-01-03T11:47:49.702135Z",
          "iopub.status.idle": "2026-01-03T11:47:50.412081Z",
          "shell.execute_reply.started": "2026-01-03T11:47:49.702113Z",
          "shell.execute_reply": "2026-01-03T11:47:50.411225Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alternative models\n",
        "\n",
        "You can experiment with alternative models as well. Uncomment the following cell to use alternative model\n",
        "\n",
        "- **microsoft/DialoGPT-small** (GPT-2 based but chat-trained)\n",
        "- **meta-llama/Llama-2-Chat**\n",
        "- **mistralai/Mistral-7B-Instruct**\n",
        "- **Qwen/Qwen-Chat**"
      ],
      "metadata": {
        "id": "YWUdqSvI3fzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "##  Uncomment this code to use alternative models\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-05T12:46:37.695693Z",
          "iopub.execute_input": "2026-01-05T12:46:37.695982Z",
          "iopub.status.idle": "2026-01-05T12:46:37.699648Z",
          "shell.execute_reply.started": "2026-01-05T12:46:37.695958Z",
          "shell.execute_reply": "2026-01-05T12:46:37.698915Z"
        },
        "id": "YmwZbym23fzb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "Text data must be converted into numbers for the model to understand.\n",
        "\n",
        "#### Tokenization Steps\n",
        "- Convert text into token IDs\n",
        "- Truncate or pad sequences to a fixed length\n",
        "- Create labels for supervised learning"
      ],
      "metadata": {
        "id": "TNftDpcp3fzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    tokens = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "tokenized_ds = dataset.map(tokenize, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "98c7HWWRuvaT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-05T12:31:44.113913Z",
          "iopub.execute_input": "2026-01-05T12:31:44.114504Z",
          "iopub.status.idle": "2026-01-05T12:31:44.134449Z",
          "shell.execute_reply.started": "2026-01-05T12:31:44.114475Z",
          "shell.execute_reply": "2026-01-05T12:31:44.133484Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training base model on dataset\n",
        "The model is trained using supervised learning.\n",
        "- The AI predicts the next word in a sentence\n",
        "- Predictions are compared with the correct answer\n",
        "- Errors are used to improve the model\n",
        "- Training runs for multiple epochs\n",
        "\n",
        "The process gradually improves the chatbotâ€™s responses.\n",
        "Increase training epoch or add more data to improve chatbot's response"
      ],
      "metadata": {
        "id": "9eyxfF1NfpXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./chatbot\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "tg5uBGfguvdM",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T11:48:03.458085Z",
          "iopub.execute_input": "2026-01-03T11:48:03.458352Z",
          "iopub.status.idle": "2026-01-03T11:55:08.056326Z",
          "shell.execute_reply.started": "2026-01-03T11:48:03.458327Z",
          "shell.execute_reply": "2026-01-03T11:55:08.055396Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save our trained model for later use\n",
        "\n",
        "This code saves our trained model on disk so we can load and chat with it later."
      ],
      "metadata": {
        "id": "nMt9yWlXUpWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"./my_gpt2_model\"  # any folder you like\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "LG8DhK4MUkno",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T11:55:08.057687Z",
          "iopub.execute_input": "2026-01-03T11:55:08.058014Z",
          "iopub.status.idle": "2026-01-03T11:55:09.138342Z",
          "shell.execute_reply.started": "2026-01-03T11:55:08.057982Z",
          "shell.execute_reply": "2026-01-03T11:55:09.137734Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets chat with our AI\n",
        "\n",
        "#### Interface Features\n",
        "- Text-based chat interaction\n",
        "- Maintains short conversation history\n",
        "- Displays AI-generated responses instantly\n",
        "\n",
        "#### Conversation Memory Handling\n",
        "The chatbot remembers recent messages to maintain context.\n",
        "\n",
        "#### How It Works\n",
        "- Keeps a limited number of past interactions\n",
        "- Formats them into a structured prompt\n",
        "- Feeds the prompt into the model for response generation\n",
        "\n",
        "This makes conversations feel more natural."
      ],
      "metadata": {
        "id": "3BPKDYABfuan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "MAX_HISTORY = 10\n",
        "\n",
        "def build_prompt(message, history):\n",
        "    history = history[-MAX_HISTORY:]\n",
        "\n",
        "    prompt = \"\"\n",
        "    for user, bot in history:\n",
        "        prompt += f\"User: {user}\\nBot: {bot} {END_TOKEN}\\n\"\n",
        "\n",
        "    prompt += f\"User: {message}\\nBot:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def chat(message, history):\n",
        "    history = history[-MAX_HISTORY:]\n",
        "\n",
        "    prompt = build_prompt(message, history)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    end_token_id = tokenizer.encode(END_TOKEN, add_special_tokens=False)[0]\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=350,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        eos_token_id=end_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the last bot reply\n",
        "    bot_reply = decoded.split(\"Bot:\")[-1]\n",
        "    bot_reply = bot_reply.split(END_TOKEN)[0].strip()\n",
        "\n",
        "    return bot_reply\n",
        "\n",
        "\n",
        "ui_interface = gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    title=\"Tiny AI Bot\",\n",
        "    description=\"Trained on our dataset\",\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "ui_interface.launch()"
      ],
      "metadata": {
        "id": "pfkIzlBihQU8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T11:55:09.139183Z",
          "iopub.execute_input": "2026-01-03T11:55:09.139488Z",
          "iopub.status.idle": "2026-01-03T11:55:10.350087Z",
          "shell.execute_reply.started": "2026-01-03T11:55:09.139466Z",
          "shell.execute_reply": "2026-01-03T11:55:10.349506Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "LSPu_-9j3fzl"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}